#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
@author: isabellegarnreiter

This code uses the output from filter_STORMdata.py and creates a dataframe, where each row corresponds to data from one cluster, identified in the 647nm channel. 

Current rows:
'FileName': filename of the acquisition
'Date': date at which the acquisition was made
'647nm': Marker in the 647nm channel
'680nm': marker in the 680nm channel
'DIV': number of days in vitro
'Acquisition': acqusition number 
'ROI label': Roi number 
'ROI': bounding box of 3D ROI mask
'points': list of points in the cluster
'centroid': centroid coordinates of the point cloud
'NN_647_to_680': list of nearest neighbour distances for each point in the 647nm channel to points in the 680nmchannel
'NN_647_to_680_ind': indexes of selected points in the 680nm channels from the original input list
'NN_647_to_647': list of nearest neighbour distances between points in the 647nm channel
'centroid_dist_647_to_680': nearest neighbour distance from the centroid to the 680nm channel
'volume': volume of the cluster - calculated from the mask
'spherecity': spherecity of the cluster - calculated from the mask

"""


import numpy as np
import pandas as pd
from glob import glob
import functions as fcts
from scipy.spatial import KDTree
import os
import re


#target_dir =  filedialog.askdirectory()
target_dir = '/Volumes/Seagate/Elisa_STORM/Good_Images'
filename = 'storm_data' 

#name of the widefield image in each folder. 
widefield_image = ''

#folder path in which to find the demixed storm files.
#Old data is CellZone, new data is Acquisition
glob_folder_pattern = '*/Acquisition*/*emix'

#what to match the files by to create the dataframe. If you want to use all the files in the directory, leave as an empty string.
match = ''

#update lists for markers and DIVs
markers = ['spon647', 'dep647', 'bassoon680', 'rimbp680']
DIVs = ['6DIV', '8DIV', '10DIV', '13DIV']


params = fcts.get_default_params()

storm_data = pd.DataFrame(columns = ['FileName', 
                                     'Date', 
                                     '647nm', 
                                     '680nm', 
                                     'DIV', 
                                     'Acquisition', 
                                     'ROI label', 
                                     'ROI', 
                                     'points', 
                                     'centroid',
                                     'NN_647_to_680', 
                                     'NN_647_to_680_ind',
                                     'NN_647_to_647',
                                     'centroid_dist_647_to_680',
                                     'volume', 
                                     'spherecity'])



# usable_exp = pd.read_csv('/users/isabellegarnreiter/documents/vesicleSTORM/data/STORM_binary_list.csv',encoding='latin', sep=',').to_numpy()
# filename  = usable_exp[:,0]+'_'+usable_exp[:,1]
# files_infos = dict(zip(filename, usable_exp[:,2:]))


date_pattern = re.compile(r'^\d')
        
i = 0
for folder in os.listdir(target_dir):
    folder_path = os.path.join(target_dir, folder)
    # Check if folder is a directory and if the name starts with a date
    if os.path.isdir(folder_path) and re.compile(r'^\d').match(folder) and match in folder:
        print(folder)

        Demix_folders = glob(folder_path + glob_folder_pattern)
        for demix in Demix_folders:
            if os.path.isdir(demix):
                #MAKE SURE YOU HAVE THE RIGHT CHANNEL HERE AS WELL.
                channel1 = glob(demix + '*/*w1*.csv')[0]
                channel2 = glob(demix + '*/*w2*.csv')[0]
                data_in_680 = pd.read_csv(channel2)[['x [nm]', 'y [nm]', 'z [nm]']].to_numpy(dtype=np.float64)
                data_in_680[:,2] +=550

            points_647_file = os.path.join(demix, 'data/', 'points_647.npy')
            clusters_647_file = os.path.join(demix, 'data/', 'clusters_647.npy')
            points_647 = np.load(points_647_file, allow_pickle=True)[()]
            clusters_647 = np.load(clusters_647_file, allow_pickle=True)[()]

            Date = folder[:6]
            marker = [x for x in markers if x in folder]
            marker647 = [x for x in marker if '647' in x]
            marker680 = [x for x in marker if '680' in x]
            DIV = [x for x in DIVs if x in folder][0]
            acquisition = demix.split('/')[-2]

            for nb in points_647.keys():
                i=i+1
                ROI_label = int(nb)
                cluster_647 = clusters_647[nb]
                point_647 = points_647[nb]

                data_680_dist = KDTree(data_in_680)
                nn_647_680, nearest_ind_680 = data_680_dist.query(point_647, k=1) 
                
                centroid = np.mean(point_647, axis=0)
                nn_centroid647_680, nni_centroid647_680 = data_680_dist.query(centroid, k=1) 
                
                self_dist = KDTree(point_647)        
                nn_647_647, nearest_ind_647 = self_dist.query(point_647, k=2) 
                
                volume, sphericity = fcts.calculate_volume_sphericity(cluster_647, params)
                
                storm_data.loc[i] = [folder, 
                        Date, 
                        marker647, 
                        marker680, 
                        DIV, 
                        acquisition, 
                        ROI_label, 
                        cluster_647, 
                        point_647, 
                        centroid,
                        nn_647_680, 
                        nearest_ind_680, 
                        nn_647_647,
                        nn_centroid647_680,
                        volume, 
                        sphericity]


storm_data['point count'] = storm_data['points'].apply(lambda arr:arr.shape[0])
storm_data['mean_coloc_680'] = storm_data['NN_647_to_680'].apply(lambda arr:arr.mean())
storm_data['stderror_coloc_680'] = storm_data['NN_647_to_680'].apply(lambda arr:arr.std())
storm_data['mean_coloc_647'] = storm_data['NN_647_to_647'].apply(lambda arr:arr.mean())
storm_data['stderror_coloc_647'] = storm_data['NN_647_to_647'].apply(lambda arr:arr.std())

storm_data['mean_coloc_680'] = np.nan
storm_data['stderror_coloc_680'] = np.nan
storm_data['mean_coloc_647'] = np.nan
storm_data['stderror_coloc_647'] = np.nan


#add the values to the DataFrame
for i, row in storm_data.iterrows():
    storm_data.loc[i, 'mean_coloc_680'] = row['NN_647_to_680'].mean()
    storm_data.loc[i, 'stderror_coloc_680'] = row['NN_647_to_680'].std()
    storm_data.loc[i, 'mean_coloc_647'] = row['NN_647_to_647'].mean()
    storm_data.loc[i, 'stderror_coloc_647'] = row['NN_647_to_647'].std()

storm_data.to_pickle(os.path.join(target_dir, f'{filename}.pkl'))  
